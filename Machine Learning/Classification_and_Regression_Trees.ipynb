{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceType": "datasetVersion",
          "sourceId": 196262,
          "datasetId": 84803,
          "databundleVersionId": 207468
        }
      ],
      "dockerImageVersionId": 30887,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression\n",
        "- **Usage**: The cost function that is minimized to choose split points is the sum squared error across all training samples that fall within the rectangle.\n",
        "- **Formula**:\n",
        "$$SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
        "- **Example**: Suppose we have a dataset where the target variable is the price of houses based on different features like size, number of rooms, etc. We can use regression to predict the price of a new house by minimizing the sum squared error between the predicted prices and actual prices of training samples.\n",
        "\n",
        "# Classification\n",
        "- **Usage**: The Gini cost function is used to provide an indication of how pure the nodes are. Node purity refers to how mixed the training data assigned to each node is.\n",
        "- **Formula**:\n",
        "$$Gini = 1 - \\sum_{k=1}^{K} p_k^2$$\n",
        "- **Example**: Suppose we have a dataset of flowers classified into different species based on features like petal length, petal width, etc. The Gini index can be used to evaluate the splits in the dataset and ensure that the nodes are pure, meaning that the flowers in each node are primarily of one species.\n",
        "\n",
        "## Gini Index\n",
        "- **Definition**: The Gini index evaluates splits in the dataset.\n",
        "- **Usage**: It involves one input attribute and its value to divide training patterns into two groups of rows.\n",
        "- **Gini Score**:\n",
        "  - Indicates how mixed the classes are in the two groups created by the split.\n",
        "  - A perfect separation has a Gini score of 0.\n",
        "  - The worst-case split (50/50 classes in each group) has a Gini score of 1.0 (for a 2 class problem).\n",
        "- **Formula**:\n",
        "$$Gini = 1 - \\sum_{k=1}^{K} p_k^2$$\n"
      ],
      "metadata": {
        "id": "74opOxnxeBML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formulas:\n",
        "### Proportion:\n",
        "The formula for the proportion is:\n",
        "$$Proportion = \\frac{count(\\text{class value})}{count(\\text{rows})}$$\n",
        "### Gini Index:\n",
        "The formula for the Gini index is:\n",
        "$$Gini = \\sum_{i=1}^{n} (proportion_i \\times (1.0 - proportion_i))$$\n"
      ],
      "metadata": {
        "id": "-8s80asieBMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Build a Tree\n",
        "\n",
        "Creating the root node of the tree is simple using the `get split()` function on the entire dataset. Adding more nodes involves three main parts:\n",
        "1. **Terminal Nodes**\n",
        "2. **Recursive Splitting**\n",
        "3. **Building a Tree**\n",
        "\n",
        "#### Terminal Nodes\n",
        "- **Stopping Criteria**: Use depth and the number of rows the node handles.\n",
        "- **Maximum Tree Depth**: Stop splitting when the maximum depth is reached to avoid overfitting.\n",
        "- **Minimum Node Records**: Stop splitting if the node has too few training patterns to avoid overfitting.\n",
        "\n",
        "### Stopping Conditions\n",
        "- If all rows belong to one group after a split, stop splitting as there's no data to split further.\n",
        "- Terminal nodes are used for final predictions by selecting the most common class value among assigned rows.\n",
        "\n"
      ],
      "metadata": {
        "id": "zz8rS-FreBMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recursive Splitting\n",
        "\n",
        "Building a decision tree involves calling the `get split()` function repeatedly on the groups created for each node. New nodes added to an existing node are called child nodes. A node may have:\n",
        "- Zero children (a terminal node)\n",
        "- One child (one side makes a prediction directly)\n",
        "- Two child nodes (left and right in the dictionary representation)\n",
        "\n",
        "Once a node is created, child nodes are created recursively on each group of data from the split by calling the same function again.\n",
        "\n",
        "#### Recursive Procedure\n",
        "This function takes a node as an argument along with the maximum depth, the minimum number of patterns in a node, and the current depth of a node. Here's how it works:\n",
        "\n",
        "1. Extract the two groups of data split by the node for use and delete them from the node.\n",
        "2. Check if either left or right group of rows is empty; if so, create a terminal node using the available records.\n",
        "3. Check if the maximum depth is reached; if so, create a terminal node.\n",
        "4. Process the left child:\n",
        "   - Create a terminal node if the group of rows is too small.\n",
        "   - Otherwise, create and add the left node in a depth-first fashion until the bottom of the tree is reached on this branch.\n",
        "5. Process the right child in the same manner, moving back up the constructed tree to the root.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pzDZDS7ReBMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UCI Banknote Authentication Dataset\n",
        "\n",
        "## Overview\n",
        "The UCI Banknote Authentication dataset is designed to distinguish between authentic (genuine) and inauthentic (forged) banknotes. The dataset contains features extracted from images of banknotes and includes the following attributes:\n",
        "\n",
        "1. **Variance** of the Wavelet Transformed image\n",
        "2. **Skewness** of the Wavelet Transformed image\n",
        "3. **Kurtosis** of the Wavelet Transformed image\n",
        "4. **Entropy** of the image\n",
        "5. **Class**: 0 for genuine and 1 for forged\n",
        "\n",
        "## Features\n",
        "- **Variance**: Measures the variation in the wavelet transformed image.\n",
        "- **Skewness**: Measures the asymmetry in the wavelet transformed image.\n",
        "- **Kurtosis**: Measures the tailedness of the wavelet transformed image.\n",
        "- **Entropy**: Measures the randomness in the image."
      ],
      "metadata": {
        "id": "nq3Phaa_eBMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dataset"
      ],
      "metadata": {
        "id": "dK2u7V8leBMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ritesaluja/bank-note-authentication-uci-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T05:01:27.131883Z",
          "iopub.execute_input": "2025-02-17T05:01:27.132147Z",
          "iopub.status.idle": "2025-02-17T05:01:41.833848Z",
          "shell.execute_reply.started": "2025-02-17T05:01:27.132113Z",
          "shell.execute_reply": "2025-02-17T05:01:41.832999Z"
        },
        "id": "iab88xIleBMK",
        "outputId": "39953d5c-f44d-40b8-b035-6a100f021eff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ritesaluja/bank-note-authentication-uci-data?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19.2k/19.2k [00:00<00:00, 19.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/ritesaluja/bank-note-authentication-uci-data/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "PiOmLMoygEZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "import os\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "    dataset = list()\n",
        "    # Get a list of all CSV files in the directory\n",
        "    csv_files = [f for f in os.listdir(filename) if f.endswith('.csv')]\n",
        "    # If there are CSV files in the directory, use the first one\n",
        "    if csv_files:\n",
        "        csv_file_path = os.path.join(filename, csv_files[0])\n",
        "        with open(csv_file_path, 'r') as file:\n",
        "            csv_reader = reader(file)\n",
        "            for row in csv_reader:\n",
        "                if not row:\n",
        "                    continue\n",
        "                dataset.append(row)  # Add each row to the dataset list\n",
        "        return dataset\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No CSV file found in the directory.\")\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset[1:]:  # Skip header row\n",
        "        row[column] = float(row[column].strip())  # Convert each value in the specified column to float\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset[1:])  # Skip header row\n",
        "    fold_size = len(dataset_copy) // n_folds  # Ensure fold_size is an integer\n",
        "    for i in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))  # Remove the selected element from dataset_copy and add it to the fold\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for fold in folds:\n",
        "        train_set = list(folds)\n",
        "        train_set.remove(fold)  # Remove the current fold from the train_set\n",
        "        train_set = sum(train_set, [])  # Flatten the list\n",
        "        test_set = list()\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None  # Remove the target value\n",
        "        predicted = algorithm(train_set, test_set, *args)\n",
        "        actual = [row[-1] for row in fold]  # Extract the target values\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n",
        "\n",
        "# Split a dataset based on an attribute and an attribute value\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = list(), list()\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right\n",
        "\n",
        "# Calculate the Gini index for a split dataset\n",
        "def gini_index(groups, class_values):\n",
        "    gini = 0.0\n",
        "    for class_value in class_values:\n",
        "        for group in groups:\n",
        "            size = len(group)\n",
        "            if size == 0:\n",
        "                continue\n",
        "            proportion = [row[-1] for row in group].count(class_value) / float(size)\n",
        "            gini += (proportion * (1.0 - proportion))\n",
        "    return gini\n",
        "\n",
        "# Select the best split point for a dataset\n",
        "def get_split(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        for row in dataset:\n",
        "            groups = test_split(index, row[index], dataset)\n",
        "            gini = gini_index(groups, class_values)\n",
        "            if gini < b_score:\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
        "\n",
        "# Create a terminal node value\n",
        "def to_terminal(group):\n",
        "    outcomes = [row[-1] for row in group]\n",
        "    return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Create child splits for a node or make terminal\n",
        "def split(node, max_depth, min_size, depth):\n",
        "    left, right = node['groups']\n",
        "    del(node['groups'])\n",
        "    # Check for a no split\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "    # Check for max depth\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "    # Process left child\n",
        "    if len(left) <= min_size:\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:\n",
        "        node['left'] = get_split(left)\n",
        "        split(node['left'], max_depth, min_size, depth+1)\n",
        "    # Process right child\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right)\n",
        "        split(node['right'], max_depth, min_size, depth+1)\n",
        "\n",
        "# Build a decision tree\n",
        "def build_tree(train, max_depth, min_size):\n",
        "    root = get_split(train)\n",
        "    split(root, max_depth, min_size, 1)\n",
        "    return root\n",
        "\n",
        "# Make a prediction with a decision tree\n",
        "def predict(node, row):\n",
        "    if row[node['index']] < node['value']:\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict(node['left'], row)\n",
        "        else:\n",
        "            return node['left']\n",
        "    else:\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict(node['right'], row)\n",
        "        else:\n",
        "            return node['right']\n",
        "\n",
        "# Classification and Regression Tree Algorithm\n",
        "def decision_tree(train, test, max_depth, min_size):\n",
        "    tree = build_tree(train, max_depth, min_size)\n",
        "    predictions = list()\n",
        "    for row in test:\n",
        "        prediction = predict(tree, row)\n",
        "        predictions.append(prediction)\n",
        "    return(predictions)\n",
        "\n",
        "# Test CART on Bank Note dataset\n",
        "seed(1)\n",
        "# Load and prepare data\n",
        "filename = '/root/.cache/kagglehub/datasets/ritesaluja/bank-note-authentication-uci-data/versions/1'\n",
        "dataset = load_csv(filename)\n",
        "# Convert string attributes to integers\n",
        "for i in range(len(dataset[0])):\n",
        "    str_column_to_float(dataset, i)\n",
        "# Evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 5\n",
        "min_size = 10\n",
        "scores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T05:47:07.52802Z",
          "iopub.execute_input": "2025-02-17T05:47:07.528349Z",
          "iopub.status.idle": "2025-02-17T05:47:17.451603Z",
          "shell.execute_reply.started": "2025-02-17T05:47:07.528324Z",
          "shell.execute_reply": "2025-02-17T05:47:17.450668Z"
        },
        "id": "E_j0sg8YeBMO",
        "outputId": "5d67fd08-274c-40a4-add5-04afb9c6764d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [83.57664233576642, 82.84671532846716, 86.86131386861314, 79.92700729927007, 82.11678832116789]\n",
            "Mean Accuracy: 83.066%\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "jk-VsxyoeBMQ"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}